# ğŸ”„ Customer Churn Analysis & Prediction

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.3+-orange.svg)
![XGBoost](https://img.shields.io/badge/XGBoost-2.0+-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## ğŸ“‹ Tá»•ng quan (Overview)

Dá»± Ã¡n nÃ y khÃ´ng chá»‰ lÃ  má»™t bÃ i toÃ¡n phÃ¢n loáº¡i Machine Learning thÃ´ng thÆ°á»ng. ÄÃ¢y lÃ  má»™t há»‡ thá»‘ng **Software Engineering for Data Science** hoÃ n chá»‰nh, giáº£i quyáº¿t bÃ i toÃ¡n dá»± Ä‘oÃ¡n khÃ¡ch hÃ ng rá»i bá» (Customer Churn) cho lÄ©nh vá»±c ThÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ (E-Commerce).

> ğŸ’¡ **KhÃ¡c biá»‡t chÃ­nh**: Thay vÃ¬ cháº¡y code trÃªn Jupyter Notebook rá»i ráº¡c, há»‡ thá»‘ng nÃ y Ä‘Æ°á»£c xÃ¢y dá»±ng thÃ nh má»™t **Pipeline khÃ©p kÃ­n**, cÃ³ kháº£ nÄƒng tÃ¡i sá»­ dá»¥ng (reproducible), dá»… dÃ ng má»Ÿ rá»™ng (scalable) vÃ  tÃ­ch há»£p sáºµn quy trÃ¬nh **MLOps tá»± xÃ¢y dá»±ng** (Custom MLOps).

### ğŸ’¼ GiÃ¡ Trá»‹ Kinh Doanh (Business Value)

| GiÃ¡ trá»‹ | MÃ´ táº£ |
|---------|-------|
| ğŸ¯ **SÃ ng lá»c sá»›m** | Nháº­n diá»‡n khÃ¡ch hÃ ng cÃ³ nguy cÆ¡ rá»i bá» vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao (F1-Score > 0.85) |
| ğŸ” **Hiá»ƒu hÃ nh vi** | Sá»­ dá»¥ng SHAP Ä‘á»ƒ giáº£i thÃ­ch lÃ½ do khÃ¡ch hÃ ng rá»i bá» (VD: Do thá»i gian giao hÃ ng, hay do Ã­t nháº­n Ä‘Æ°á»£c Æ°u Ä‘Ã£i) |
| ğŸ’° **Tá»‘i Æ°u chi phÃ­** | GiÃºp bá»™ pháº­n Marketing khoanh vÃ¹ng Ä‘Ãºng Ä‘á»‘i tÆ°á»£ng Ä‘á»ƒ gá»­i voucher giá»¯ chÃ¢n, trÃ¡nh lÃ£ng phÃ­ ngÃ¢n sÃ¡ch |
| ğŸ“¦ **Quáº£n trá»‹ mÃ´ hÃ¬nh** | Version tracking cho dá»¯ liá»‡u, model registry, monitoring vÃ  health check tá»± Ä‘á»™ng |

### ğŸ“ Äáº·c Ä‘iá»ƒm ká»¹ thuáº­t ná»•i báº­t

- âœ… **Modular Architecture**: TÃ¡ch biá»‡t rÃµ rÃ ng giá»¯a Data, Model, Ops, Visualization
- âœ… **Data Leakage Prevention**: Fit trÃªn Train, Transform trÃªn Test - tuÃ¢n thá»§ nghiÃªm ngáº·t
- âœ… **Multiple Models Support**: LogisticRegression, SVM, DecisionTree, RandomForest, XGBoost, AdaBoost
- âœ… **Automated Hyperparameter Tuning**: RandomizedSearchCV vá»›i cross-validation
- âœ… **Imbalanced Data Handling**: SMOTE + Tomek Links Ä‘á»ƒ cÃ¢n báº±ng lá»›p Churn
- âœ… **Experiment Tracking**: LÆ°u trá»¯ tá»«ng run vá»›i snapshot config, metrics, models
- âœ… **Model Registry**: Quáº£n lÃ½ phiÃªn báº£n model production-ready
- âœ… **Performance Monitoring**: Health check tá»± Ä‘á»™ng, drift detection
- âœ… **Explainability**: SHAP values Ä‘á»ƒ giáº£i thÃ­ch quyáº¿t Ä‘á»‹nh cá»§a model

---

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng (System Architecture)

### ğŸ“Š Pipeline Flow - Luá»“ng xá»­ lÃ½ End-to-End

```mermaid
flowchart TB
    subgraph INPUT["ğŸ“¥ DATA INPUT"]
        A[("RAW DATA<br/>E Commerce Dataset.xlsx<br/>Sheet: E Comm")]
    end

    subgraph STAGE1["ğŸ“Š STAGE 1: EDA (Exploratory Data Analysis)"]
        B1["Load Raw Data"]
        B2["Analyze Missing Values"]
        B3["Correlation Matrix"]
        B4["Distribution Plots"]
        B5["Outlier Detection"]
        B1 --> B2 --> B3 --> B4 --> B5
    end

    subgraph STAGE2["ğŸ§¹ STAGE 2: PREPROCESSING (DataPreprocessor)"]
        C1["Clean Data<br/>â€¢ Remove duplicates<br/>â€¢ Standardize columns<br/>â€¢ Fix domain-specific errors"]
        C2["Validate Data Quality"]
        C3["Stratified Split<br/>80% Train / 20% Test"]
        C1 --> C2 --> C3
    end

    subgraph STAGE3["âš™ï¸ STAGE 3: TRANSFORMATION (DataTransformer)"]
        D1["TRAIN SET"]
        D2["FIT + TRANSFORM<br/>â€¢ Impute missing (median/mode)<br/>â€¢ Handle outliers (IQR clipping)<br/>â€¢ Feature engineering<br/>â€¢ Label encoding<br/>â€¢ Standard scaling"]
        D3["LEARNED PARAMS<br/>Imputers, Bounds,<br/>Encoders, Scaler"]
        D4["TEST SET"]
        D5["TRANSFORM ONLY<br/>(Apply learned params)"]
        D6["Processed Train"]
        D7["Processed Test"]
        
        D1 --> D2
        D2 --> D3
        D3 --> D6
        D4 --> D5
        D3 -.->|"Apply"| D5
        D5 --> D7
    end

    subgraph STAGE4["ğŸ“ STAGE 4: TRAINING (ModelTrainer)"]
        E1["Apply SMOTE + Tomek<br/>(Balance classes)"]
        E2["Train Multiple Models<br/>â€¢ Logistic Regression<br/>â€¢ SVM<br/>â€¢ Decision Tree<br/>â€¢ Random Forest<br/>â€¢ XGBoost<br/>â€¢ AdaBoost"]
        E3["Hyperparameter Tuning<br/>(RandomizedSearchCV)"]
        E4["Select Best Model<br/>(Based on F1-Score)"]
        E1 --> E2 --> E3 --> E4
    end

    subgraph STAGE5["ğŸ“ˆ STAGE 5: EVALUATION (Evaluator)"]
        F1["Confusion Matrix"]
        F2["ROC-AUC Curves"]
        F3["Feature Importance"]
        F4["Model Comparison"]
        F5["SHAP Explainability"]
        F1 --> F2 --> F3 --> F4 --> F5
    end

    subgraph STAGE6["ğŸ“¦ STAGE 6: MLOPS"]
        G1["ExperimentTracker<br/>Log params & metrics"]
        G2["ModelRegistry<br/>Save best model + metadata"]
        G3["DataVersioning<br/>Track data hash"]
        G4["ModelMonitor<br/>Health check & drift detection"]
        G1 --> G2 --> G3 --> G4
    end

    subgraph OUTPUT["ğŸ’¾ OUTPUTS"]
        H1["artifacts/experiments/<run_id>/"]
        H2["artifacts/model_registry/"]
        H3["artifacts/monitoring/"]
        H4["artifacts/versions/"]
    end

    A --> STAGE1
    STAGE1 --> STAGE2
    STAGE2 --> STAGE3
    STAGE3 --> STAGE4
    STAGE4 --> STAGE5
    STAGE5 --> STAGE6
    STAGE6 --> OUTPUT

    style INPUT fill:#e1f5fe
    style STAGE1 fill:#fff3e0
    style STAGE2 fill:#e8f5e9
    style STAGE3 fill:#f3e5f5
    style STAGE4 fill:#e0f2f1
    style STAGE5 fill:#fff8e1
    style STAGE6 fill:#e8eaf6
    style OUTPUT fill:#fbe9e7
```

### ğŸ” NguyÃªn táº¯c chá»‘ng Data Leakage

> âš ï¸ **Quan trá»ng**: Má»i thÃ´ng tin thá»‘ng kÃª (mean, std, IQR bounds, encoding mappings...) chá»‰ Ä‘Æ°á»£c há»c tá»« **Train Set**. Test Set chá»‰ Ä‘Æ°á»£c **Transform** vá»›i tham sá»‘ Ä‘Ã£ há»c.

```mermaid
flowchart LR
    subgraph TRAIN["ğŸ¯ TRAIN SET (80%)"]
        A["FIT<br/>Learn parameters from data"]
        B["TRANSFORM<br/>Apply to self"]
        A --> B
    end

    subgraph PARAMS["ğŸ“Š LEARNED PARAMETERS"]
        C["â€¢ Imputer values (median/mode)<br/>â€¢ Outlier bounds (Q1-1.5*IQR, Q3+1.5*IQR)<br/>â€¢ Label encoders mapping<br/>â€¢ Scaler params (mean, std)<br/>â€¢ Feature selection mask"]
    end
    
    subgraph TEST["ğŸ§ª TEST SET (20%)"]
        D["TRANSFORM ONLY<br/>(No fitting!)"]
    end
    
    B -->|"Store"| C
    C -->|"Apply frozen params"| D

    style TRAIN fill:#c8e6c9
    style PARAMS fill:#fff9c4
    style TEST fill:#ffcdd2
```

### ğŸ§© Kiáº¿n trÃºc module (Module Architecture)

```mermaid
graph TB
    subgraph ENTRY["ğŸšª ENTRY POINT"]
        MAIN["main.py<br/>CLI Interface"]
    end
    
    subgraph PIPELINE["ğŸ”„ PIPELINE ORCHESTRATOR"]
        PIPE["pipeline.py<br/>â€¢ run_eda()<br/>â€¢ run_preprocessing()<br/>â€¢ run_training()<br/>â€¢ run_visualization()"]
    end

    subgraph DATA["ğŸ“Š DATA MODULE"]
        PREP["preprocessor.py<br/>â€¢ load_data()<br/>â€¢ clean_data()<br/>â€¢ split_data()"]
        TRANS["transformer.py<br/>â€¢ fit_transform()<br/>â€¢ transform()<br/>â€¢ get_resampler()"]
    end
    
    subgraph MODELS["ğŸ¤– MODELS MODULE"]
        TRAIN["trainer.py<br/>â€¢ train_model()<br/>â€¢ train_all_models()<br/>â€¢ evaluate()"]
        OPT["optimizer.py<br/>â€¢ optimize_params()<br/>â€¢ grid_search()"]
        EVAL["evaluator.py<br/>â€¢ calculate_metrics()<br/>â€¢ confusion_matrix()"]
    end
    
    subgraph VIZ["ğŸ“ˆ VISUALIZATION MODULE"]
        EDA["eda_plots.py<br/>â€¢ plot_missing_values()<br/>â€¢ plot_correlation_matrix()"]
        EVIZ["evaluate_plots.py<br/>â€¢ plot_confusion_matrix()<br/>â€¢ plot_roc_curve()<br/>â€¢ plot_feature_importance()"]
    end
    
    subgraph OPS["âš¡ OPS MODULE"]
        DOPS["dataops.py<br/>â€¢ DataValidator<br/>â€¢ DataVersioning"]
        MOPS["mlops.py<br/>â€¢ ExperimentTracker<br/>â€¢ ModelRegistry<br/>â€¢ ModelMonitor<br/>â€¢ ModelExplainer"]
    end
    
    subgraph UTILS["ğŸ› ï¸ UTILITIES"]
        UTIL["utils.py<br/>â€¢ ConfigLoader<br/>â€¢ Logger<br/>â€¢ IOHandler"]
    end

    MAIN --> PIPE
    PIPE --> DATA
    PIPE --> MODELS
    PIPE --> VIZ
    PIPE --> OPS
    
    DATA --> UTIL
    MODELS --> UTIL
    VIZ --> UTIL
    OPS --> UTIL
    
    PREP -.-> TRANS
    TRAIN -.-> OPT
    TRAIN -.-> EVAL

    style ENTRY fill:#e3f2fd
    style PIPELINE fill:#f3e5f5
    style DATA fill:#e8f5e9
    style MODELS fill:#fff3e0
    style VIZ fill:#fce4ec
    style OPS fill:#fff8e1
    style UTILS fill:#eceff1
```

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c (Project Structure)

```
ğŸ“¦ Churn_Analys_and_Prediction/
â”‚
â”œâ”€â”€ ğŸ“„ main.py                           # ğŸšª Entry point chÃ­nh - CLI interface
â”œâ”€â”€ ğŸ“„ README.md                         # ğŸ“– Documentation (file nÃ y)
â”œâ”€â”€ ğŸ“„ requirements.txt                  # ğŸ“¦ Python dependencies
â”‚
â”œâ”€â”€ ğŸ“‚ config/                           # âš™ï¸ Cáº¤U HÃŒNH
â”‚   â””â”€â”€ ğŸ“„ config.yaml                   # File cáº¥u hÃ¬nh táº­p trung (paths, models, tuning params)
â”‚
â”œâ”€â”€ ğŸ“‚ src/                              # ğŸ’» MÃƒ NGUá»’N CHÃNH
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ pipeline.py                   # ğŸ”„ Orchestrator - Ä‘iá»u phá»‘i toÃ n bá»™ pipeline
â”‚   â”œâ”€â”€ ğŸ“„ utils.py                      # ğŸ› ï¸ Tiá»‡n Ã­ch: Logger, IOHandler, ConfigLoader
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ data/                         # ğŸ“Š MODULE Xá»¬ LÃ Dá»® LIá»†U
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ preprocessor.py           # Giai Ä‘oáº¡n 1: Load, Clean, Split (Stateless)
â”‚   â”‚   â””â”€â”€ ğŸ“„ transformer.py            # Giai Ä‘oáº¡n 2: Transform, Feature Engineering (Stateful)
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ models/                       # ğŸ¤– MODULE MÃ” HÃŒNH
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ trainer.py                # Train models, evaluate, select best
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ optimizer.py              # Hyperparameter tuning (RandomizedSearch)
â”‚   â”‚   â””â”€â”€ ğŸ“„ evaluator.py              # Metrics calculation, evaluation logic
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ ops/                          # âš¡ MODULE MLOPS
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dataops.py                # DataValidator, DataVersioning
â”‚   â”‚   â””â”€â”€ ğŸ“„ mlops.py                  # ExperimentTracker, ModelRegistry, Monitor, Explainer
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ visualization/                # ğŸ“ˆ MODULE VISUALIZATION
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“„ eda_plots.py              # EDA visualizations (missing, correlation, distribution)
â”‚       â””â”€â”€ ğŸ“„ evaluate_plots.py         # Model evaluation plots (confusion matrix, ROC, feature importance)
â”‚
â”œâ”€â”€ ğŸ“‚ data/                             # ğŸ’¾ Dá»® LIá»†U LÃ€M VIá»†C (WORKSPACE)
â”‚   â”œâ”€â”€ ğŸ“‚ raw/                          # Dá»¯ liá»‡u thÃ´ gá»‘c
â”‚   â”‚   â””â”€â”€ ğŸ“„ E Commerce Dataset.xlsx   # File Excel input chÃ­nh
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ processed/                    # Dá»¯ liá»‡u Ä‘Ã£ clean (intermediate)
â”‚   â”‚   â””â”€â”€ ğŸ“„ E Commerce Dataset_cleaned.parquet
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ train_test/                   # Dá»¯ liá»‡u Ä‘Ã£ split vÃ  transform (ready for training)
â”‚       â”œâ”€â”€ ğŸ“„ E Commerce Dataset_train.parquet
â”‚       â””â”€â”€ ğŸ“„ E Commerce Dataset_test.parquet
â”‚
â”œâ”€â”€ ğŸ“‚ artifacts/                        # ğŸ—„ï¸ OUTPUTS & ARCHIVE
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ experiments/                  # ğŸ”¬ EXPERIMENT TRACKING
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ experiments.csv           # Master log cá»§a táº¥t cáº£ runs
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ ğŸ“‚ <run_id>/                 # VD: 20251207_153322_FULL/
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ config_snapshot.yaml  # Snapshot cáº¥u hÃ¬nh cá»§a run nÃ y
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ params.json           # Hyperparameters Ä‘Ã£ sá»­ dá»¥ng
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ metrics.json          # Káº¿t quáº£ metrics (F1, AUC, Precision, Recall...)
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ run.log               # Log chi tiáº¿t cá»§a run
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€â”€ ğŸ“‚ data/                 # Data snapshots
â”‚   â”‚       â”‚   â”œâ”€â”€ ğŸ“„ train.parquet     # Train set Ä‘Ã£ transform
â”‚   â”‚       â”‚   â””â”€â”€ ğŸ“„ test.parquet      # Test set Ä‘Ã£ transform
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€â”€ ğŸ“‚ models/               # Models artifacts
â”‚   â”‚       â”‚   â”œâ”€â”€ ğŸ“„ transformer.joblib    # DataTransformer (cáº§n cho inference!)
â”‚   â”‚       â”‚   â””â”€â”€ ğŸ“„ xgboost.joblib        # Best model cá»§a run
â”‚   â”‚       â”‚
â”‚   â”‚       â””â”€â”€ ğŸ“‚ figures/              # Visualizations
â”‚   â”‚           â”œâ”€â”€ ğŸ“‚ eda/              # EDA plots (missing, correlation, distribution)
â”‚   â”‚           â”‚   â”œâ”€â”€ ğŸ“„ missing_values.png
â”‚   â”‚           â”‚   â”œâ”€â”€ ğŸ“„ correlation_matrix.png
â”‚   â”‚           â”‚   â””â”€â”€ ğŸ“„ numerical_distributions.png
â”‚   â”‚           â”‚
â”‚   â”‚           â””â”€â”€ ğŸ“‚ evaluation/       # Model evaluation plots
â”‚   â”‚               â”œâ”€â”€ ğŸ“„ confusion_matrix_xgboost.png
â”‚   â”‚               â”œâ”€â”€ ğŸ“„ roc_curve.png
â”‚   â”‚               â”œâ”€â”€ ğŸ“„ feature_importance.png
â”‚   â”‚               â”œâ”€â”€ ğŸ“„ model_comparison.png
â”‚   â”‚               â””â”€â”€ ğŸ“„ shap_summary.png
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ model_registry/               # ğŸ“¦ MODEL REGISTRY (Production Models)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ registry.json             # Metadata: model versions, metrics, run_id
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ xgboost_v1_20251207_154104.joblib
â”‚   â”‚   â””â”€â”€ ğŸ“„ xgboost_v2_20251207_160224.joblib
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ monitoring/                   # ğŸ‘ï¸ MODEL MONITORING
â”‚   â”‚   â””â”€â”€ ğŸ“„ performance_log.csv       # Log hiá»‡u nÄƒng theo thá»i gian (tracking drift, degradation)
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ versions/                     # ğŸ”– DATA VERSIONING
â”‚   â”‚   â””â”€â”€ ğŸ“„ versions.json             # Hash vÃ  metadata cá»§a cÃ¡c phiÃªn báº£n dá»¯ liá»‡u
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ figures/                      # ğŸ“Š Latest figures (symbolic links hoáº·c copy)
â”‚   â””â”€â”€ ğŸ“‚ logs/                         # ğŸ“ Global logs
â”‚       â””â”€â”€ ğŸ“„ MAIN_20251207.log
â”‚
â””â”€â”€ ğŸ“‚ tests/                            # ğŸ§ª TESTING SUITE
    â”œâ”€â”€ ğŸ“„ conftest.py                   # Pytest fixtures vÃ  configuration
    â”œâ”€â”€ ğŸ“„ test_data_processing.py       # Tests cho data module
    â”œâ”€â”€ ğŸ“„ test_feature_engineering.py   # Tests cho transformer
    â”œâ”€â”€ ğŸ“„ test_model_training.py        # Tests cho models module
    â”œâ”€â”€ ğŸ“„ test_utils.py                 # Tests cho utilities
    â”‚
    â””â”€â”€ ğŸ“‚ data/                         # Test data samples
        â”œâ”€â”€ ğŸ“„ sample_raw.csv            # Raw data máº«u cho testing
        â””â”€â”€ ğŸ“„ sample_processed.csv      # Processed data máº«u
```

### ğŸ“‹ Giáº£i thÃ­ch cÃ¡c thÃ nh pháº§n quan trá»ng

| ThÆ° má»¥c/File | Má»¥c Ä‘Ã­ch | Khi nÃ o cáº§n |
|--------------|----------|-------------|
| `config/config.yaml` | Cáº¥u hÃ¬nh táº­p trung cho toÃ n bá»™ pipeline | Thay Ä‘á»•i paths, model params, tuning settings |
| `src/pipeline.py` | Orchestrator Ä‘iá»u phá»‘i cÃ¡c stages | Entry point logic cho cÃ¡c modes (eda, train, full...) |
| `src/data/preprocessor.py` | Clean vÃ  split data (stateless) | Xá»­ lÃ½ dá»¯ liá»‡u thÃ´ ban Ä‘áº§u |
| `src/data/transformer.py` | Feature engineering (stateful) | Há»c tham sá»‘ tá»« train, apply cho test |
| `src/models/trainer.py` | Training logic | Train vÃ  evaluate models |
| `src/ops/mlops.py` | MLOps components | Tracking, registry, monitoring |
| `artifacts/experiments/` | LÆ°u trá»¯ tá»«ng run | Review láº¡i experiments cÅ© |
| `artifacts/model_registry/` | Models production-ready | Deploy model vÃ o production |
| `artifacts/monitoring/` | Performance logs | Theo dÃµi model degradation |
| `tests/` | Unit & integration tests | CI/CD, Ä‘áº£m báº£o code quality |

---

## âš™ï¸ Cáº¥u hÃ¬nh há»‡ thá»‘ng (Configuration)

Táº¥t cáº£ cáº¥u hÃ¬nh Ä‘Æ°á»£c táº­p trung trong `config/config.yaml`. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c sections chÃ­nh:

### ğŸ“Š Data Configuration
```yaml
data:
  target_col: "Churn"                              # Cá»™t target cáº§n dá»± Ä‘oÃ¡n
  date_col: "DaySinceLastOrder"                    # Cá»™t ngÃ y thÃ¡ng (náº¿u cÃ³)
  raw_path: "data/raw/E Commerce Dataset.xlsx"     # ÄÆ°á»ng dáº«n file input
  sheet_name: "E Comm"                             # TÃªn sheet Excel
  batch_folder: "data/raw"                         # ThÆ° má»¥c chá»©a nhiá»u files (batch processing)
  test_size: 0.2                                   # Tá»· lá»‡ test set (20%)
  random_state: 42                                 # Seed cho reproducibility
```

### ğŸ”§ Preprocessing Configuration
```yaml
preprocessing:
  clean:
    remove_duplicates: true                        # Loáº¡i bá» dÃ²ng trÃ¹ng láº·p
    standardize_values: true                       # Chuáº©n hÃ³a giÃ¡ trá»‹ (lowercase, strip...)
  
  missing_strategy:
    numerical: "median"                            # Äiá»n giÃ¡ trá»‹ khuyáº¿t: median cho sá»‘
    categorical: "mode"                            # Äiá»n giÃ¡ trá»‹ khuyáº¿t: mode cho categorical
  
  outlier_method: "iqr"                           # PhÆ°Æ¡ng phÃ¡p xá»­ lÃ½ outliers: IQR
  outlier_threshold: 1.5                          # NgÆ°á»¡ng IQR (Q1-1.5*IQR, Q3+1.5*IQR)
  
  scaler_type: "standard"                         # Loáº¡i scaler: standard, minmax, robust
  categorical_encoding: "label"                   # Encoding: label, onehot
  
  create_features: true                           # Táº¡o features má»›i
  feature_selection: true                         # Lá»c features quan trá»ng
  feature_selection_method: "f_classif"           # PhÆ°Æ¡ng phÃ¡p: f_classif, mutual_info
  n_top_features: 15                              # Sá»‘ features giá»¯ láº¡i
  
  use_smote: true                                 # Sá»­ dá»¥ng SMOTE Ä‘á»ƒ balance classes
  k_neighbors: 5                                  # Sá»‘ neighbors cho SMOTE
  use_tomek: true                                 # Káº¿t há»£p Tomek Links (clean boundaries)
```

### ğŸ¤– Models Configuration
```yaml
models:
  logistic_regression:
    C: [0.001, 0.01, 0.1, 1, 10]
    penalty: ["l2"]
    solver: ["lbfgs", "liblinear"]
    max_iter: [1000]

  random_forest:
    n_estimators: [50, 100, 200]
    max_depth: [10, 20, null]
    min_samples_split: [2, 5]
    min_samples_leaf: [1, 2]

  xgboost:
    n_estimators: [100, 300, 500]
    max_depth: [3, 5, 7]
    learning_rate: [0.01, 0.05, 0.1]
    eval_metric: ["logloss"]
```

### ğŸ” Tuning Configuration
```yaml
tuning:
  method: "randomized"                            # PhÆ°Æ¡ng phÃ¡p: grid, randomized
  cv_folds: 5                                     # Sá»‘ folds cho cross-validation
  cv_strategy: "stratified"                       # Stratified Ä‘á»ƒ giá»¯ tá»· lá»‡ classes
  n_iter: 20                                      # Sá»‘ iterations cho RandomizedSearch
  scoring: "f1"                                   # Metric chÃ­nh Ä‘á»ƒ optimize
  n_jobs: -1                                      # Sá»­ dá»¥ng táº¥t cáº£ CPU cores
```

### ğŸ“¦ MLOps Configuration
```yaml
experiments:
  enabled: true
  base_dir: "artifacts/experiments"
  experiments_file: "experiments.csv"

mlops:
  registry_dir: "artifacts/model_registry"

monitoring:
  enabled: true
  base_dir: "artifacts/monitoring"
  performance_log: "performance_log.csv"
  health_check:
    f1_min: 0.70                                  # F1 tá»‘i thiá»ƒu cháº¥p nháº­n Ä‘Æ°á»£c
    accuracy_min: 0.75                            # Accuracy tá»‘i thiá»ƒu
    drift_max: 0.10                               # Drift tá»‘i Ä‘a cho phÃ©p (10%)

explainability:
  enabled: true
  methods: ["shap"]
  shap_samples: 100                               # Sá»‘ samples dÃ¹ng cho SHAP
```

---

## ğŸš€ HÆ°á»›ng dáº«n cÃ i Ä‘áº·t vÃ  sá»­ dá»¥ng (Installation & Usage)

### ğŸ“¥ BÆ°á»›c 1: Clone Repository

```powershell
git clone https://github.com/civi0411/Churn_Analys_and_Prediction.git
cd Churn_Analys_and_Prediction
```

### ğŸ BÆ°á»›c 2: Táº¡o Virtual Environment (Khuyáº¿n nghá»‹)

**Windows (PowerShell):**
```powershell
# Táº¡o virtual environment
python -m venv .venv

# KÃ­ch hoáº¡t virtual environment
.\.venv\Scripts\Activate.ps1

# Náº¿u gáº·p lá»—i ExecutionPolicy:
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

**Linux/MacOS:**
```bash
# Táº¡o virtual environment
python3 -m venv .venv

# KÃ­ch hoáº¡t
source .venv/bin/activate
```

### ğŸ“¦ BÆ°á»›c 3: CÃ i Ä‘áº·t Dependencies

```powershell
# Upgrade pip
python -m pip install --upgrade pip

# CÃ i Ä‘áº·t táº¥t cáº£ packages
pip install -r requirements.txt

# Kiá»ƒm tra cÃ i Ä‘áº·t thÃ nh cÃ´ng
pip list
```

**Dependencies chÃ­nh:**
- `pandas>=2.0.3`, `numpy>=1.24.3` - Xá»­ lÃ½ dá»¯ liá»‡u
- `scikit-learn>=1.3.0` - Machine Learning
- `xgboost>=2.0.0` - Gradient Boosting
- `imbalanced-learn>=0.11.0` - SMOTE
- `matplotlib>=3.7.2`, `seaborn>=0.12.2` - Visualization
- `shap>=0.42.1` - Model explainability
- `PyYAML>=6.0.1` - Config parsing
- `pytest>=9.0.0`, `pytest-cov>=7.0.0` - Testing

### âš™ï¸ BÆ°á»›c 4: Cáº¥u hÃ¬nh (Optional)

Chá»‰nh sá»­a `config/config.yaml` náº¿u cáº§n:
```yaml
# Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n data
data:
  raw_path: "data/raw/your_data.xlsx"
  sheet_name: "YourSheet"

# Äiá»u chá»‰nh tham sá»‘ models
models:
  xgboost:
    n_estimators: [100, 200]  # Giáº£m Ä‘á»ƒ cháº¡y nhanh hÆ¡n
```

---

## ğŸ¯ CÃ¡ch cháº¡y Pipeline (Running the Pipeline)

### 1ï¸âƒ£ **Mode: EDA (Exploratory Data Analysis)**

PhÃ¢n tÃ­ch dá»¯ liá»‡u thÃ´, táº¡o cÃ¡c biá»ƒu Ä‘á»“ trá»±c quan.

```powershell
python main.py --mode eda
```

**Output:**
- `artifacts/experiments/<run_id>/figures/eda/`
  - `missing_values.png` - Biá»ƒu Ä‘á»“ missing values
  - `correlation_matrix.png` - Ma tráº­n tÆ°Æ¡ng quan
  - `numerical_distributions.png` - PhÃ¢n phá»‘i cÃ¡c biáº¿n sá»‘
  - `target_distribution.png` - PhÃ¢n phá»‘i target (Churn)
  - `outliers_boxplot.png` - Boxplot phÃ¡t hiá»‡n outliers

**Use case:** Hiá»ƒu dá»¯ liá»‡u trÆ°á»›c khi preprocessing

---

### 2ï¸âƒ£ **Mode: Preprocess (Data Preprocessing)**

LÃ m sáº¡ch, split vÃ  transform dá»¯ liá»‡u.

```powershell
python main.py --mode preprocess
```

**CÃ¡c bÆ°á»›c thá»±c hiá»‡n:**
1. Load raw data tá»« Excel
2. Clean data (remove duplicates, standardize)
3. Split train/test (80/20, stratified)
4. Fit transformer trÃªn train set
5. Transform cáº£ train vÃ  test set

**Output:**
- `data/processed/E Commerce Dataset_cleaned.parquet`
- `data/train_test/E Commerce Dataset_train.parquet`
- `data/train_test/E Commerce Dataset_test.parquet`
- `artifacts/experiments/<run_id>/data/` (snapshot)

**Use case:** Chuáº©n bá»‹ dá»¯ liá»‡u sáºµn sÃ ng cho training

---

### 3ï¸âƒ£ **Mode: Train (Model Training)**

Train models vá»›i dá»¯ liá»‡u Ä‘Ã£ preprocess.

#### A. Train má»™t model cá»¥ thá»ƒ (khÃ´ng optimize)

```powershell
python main.py --mode train --model xgboost
```

#### B. Train má»™t model vá»›i hyperparameter tuning

```powershell
python main.py --mode train --model xgboost --optimize
```

#### C. Train táº¥t cáº£ models

```powershell
python main.py --mode train --model all
```

#### D. Train táº¥t cáº£ models + optimize

```powershell
python main.py --mode train --model all --optimize
```

**Models há»— trá»£:**
- `logistic_regression` - Baseline model
- `svm` - Support Vector Machine
- `decision_tree` - Decision Tree
- `random_forest` - Random Forest Ensemble
- `xgboost` - XGBoost (thÆ°á»ng tá»‘t nháº¥t)
- `adaboost` - AdaBoost Ensemble
- `all` - Train táº¥t cáº£ models trÃªn

**Output:**
- `artifacts/experiments/<run_id>/models/xgboost.joblib` (best model)
- `artifacts/experiments/<run_id>/models/transformer.joblib` (cáº§n cho inference)
- `artifacts/experiments/<run_id>/metrics.json`
- `artifacts/model_registry/xgboost_v1_<timestamp>.joblib` (production model)

**Use case:** Training vÃ  tÃ¬m model tá»‘t nháº¥t

---

### 4ï¸âƒ£ **Mode: Visualize (Visualization Only)**

Cháº¡y quick training (khÃ´ng optimize) vÃ  táº¡o visualizations.

```powershell
python main.py --mode visualize --model xgboost
```

**Output:**
- `artifacts/experiments/<run_id>/figures/evaluation/`
  - `confusion_matrix_xgboost.png`
  - `roc_curve.png`
  - `feature_importance.png`
  - `model_comparison.png`
  - `shap_summary.png` (náº¿u enabled)

**Use case:** Kiá»ƒm tra nhanh performance vÃ  feature importance

---

### 5ï¸âƒ£ **Mode: Full (End-to-End Pipeline)** â­ Khuyáº¿n nghá»‹

Cháº¡y toÃ n bá»™ pipeline tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i.

```powershell
# Full pipeline vá»›i model cá»¥ thá»ƒ + optimize
python main.py --mode full --model xgboost --optimize

# Full pipeline vá»›i táº¥t cáº£ models + optimize
python main.py --mode full --model all --optimize

# Full pipeline nhanh (khÃ´ng optimize)
python main.py --mode full --model xgboost
```

**CÃ¡c bÆ°á»›c thá»±c hiá»‡n:**
1. **EDA** - PhÃ¢n tÃ­ch dá»¯ liá»‡u thÃ´
2. **Preprocess** - Clean, split, transform
3. **Train** - Training models (vá»›i/khÃ´ng tuning)
4. **Visualize** - Táº¡o táº¥t cáº£ visualizations
5. **MLOps** - Log, registry, monitoring

**Output:** Äáº§y Ä‘á»§ táº¥t cáº£ outputs tá»« cÃ¡c modes trÃªn

**Use case:** Production pipeline hoÃ n chá»‰nh

---

### ğŸ”§ Tham sá»‘ CLI (Command-Line Arguments)

| Argument | MÃ´ táº£ | Máº·c Ä‘á»‹nh | VÃ­ dá»¥ |
|----------|-------|----------|-------|
| `--mode` | Cháº¿ Ä‘á»™ cháº¡y | `full` | `eda`, `preprocess`, `train`, `visualize`, `full` |
| `--model` | Model cá»¥ thá»ƒ | `all` | `xgboost`, `random_forest`, `logistic_regression` |
| `--optimize` | Báº­t hyperparameter tuning | `False` | `--optimize` |
| `--data` | ÄÆ°á»ng dáº«n data (override config) | `None` | `--data data/raw/new_data.xlsx` |
| `--config` | ÄÆ°á»ng dáº«n config file | `config/config.yaml` | `--config config/custom.yaml` |

### ğŸ“ VÃ­ dá»¥ thá»±c táº¿

```powershell
# 1. KhÃ¡m phÃ¡ dá»¯ liá»‡u má»›i
python main.py --mode eda --data "data/raw/new_customers.xlsx"

# 2. Train XGBoost vá»›i tuning cho production
python main.py --mode full --model xgboost --optimize

# 3. So sÃ¡nh táº¥t cáº£ models (khÃ´ng tuning - nhanh)
python main.py --mode train --model all

# 4. Cháº¡y vá»›i config tÃ¹y chá»‰nh
python main.py --mode full --config config/production.yaml --optimize

# 5. Debug: Chá»‰ visualize káº¿t quáº£
python main.py --mode visualize --model random_forest
```

---

## ğŸ§ª Testing (Kiá»ƒm thá»­)

Dá»± Ã¡n sá»­ dá»¥ng `pytest` cho unit tests vÃ  integration tests.

### ğŸ“‹ Cáº¥u trÃºc Tests

```
tests/
â”œâ”€â”€ conftest.py                      # Pytest fixtures vÃ  configuration
â”œâ”€â”€ test_data_processing.py          # Test DataPreprocessor
â”œâ”€â”€ test_feature_engineering.py      # Test DataTransformer
â”œâ”€â”€ test_model_training.py           # Test ModelTrainer
â”œâ”€â”€ test_utils.py                    # Test utilities (Logger, IOHandler, ConfigLoader)
â””â”€â”€ data/
    â”œâ”€â”€ sample_raw.csv               # Sample raw data cho tests
    â””â”€â”€ sample_processed.csv         # Sample processed data
```

### ğŸƒ Cháº¡y Tests

#### 1ï¸âƒ£ Cháº¡y táº¥t cáº£ tests

```powershell
pytest
```

**Output máº«u:**
```
============================= test session starts =============================
collected 45 items

tests/test_data_processing.py .........                                  [ 20%]
tests/test_feature_engineering.py ..........                             [ 42%]
tests/test_model_training.py ...........                                 [ 67%]
tests/test_utils.py ..............                                       [100%]

============================= 45 passed in 12.34s =============================
```

#### 2ï¸âƒ£ Cháº¡y tests vá»›i verbose output

```powershell
pytest -v
```

**Hiá»ƒn thá»‹ chi tiáº¿t tá»«ng test case:**
```
tests/test_data_processing.py::test_load_data PASSED                    [ 2%]
tests/test_data_processing.py::test_clean_data PASSED                   [ 4%]
tests/test_data_processing.py::test_split_data PASSED                   [ 6%]
...
```

#### 3ï¸âƒ£ Cháº¡y tests cá»§a má»™t module cá»¥ thá»ƒ

```powershell
# Test data processing
pytest tests/test_data_processing.py

# Test feature engineering
pytest tests/test_feature_engineering.py

# Test model training
pytest tests/test_model_training.py

# Test utilities
pytest tests/test_utils.py
```

#### 4ï¸âƒ£ Cháº¡y test cá»¥ thá»ƒ (má»™t hÃ m)

```powershell
# Cháº¡y má»™t test function cá»¥ thá»ƒ
pytest tests/test_data_processing.py::test_clean_data

# Cháº¡y tests matching pattern
pytest -k "test_load"
pytest -k "transformer"
```

#### 5ï¸âƒ£ Cháº¡y tests vá»›i coverage report

```powershell
# Coverage cÆ¡ báº£n
pytest --cov=src

# Coverage vá»›i bÃ¡o cÃ¡o chi tiáº¿t
pytest --cov=src --cov-report=term-missing

# Coverage vá»›i bÃ¡o cÃ¡o HTML
pytest --cov=src --cov-report=html

# Sau Ä‘Ã³ má»Ÿ: htmlcov/index.html
```

**Output coverage máº«u:**
```
---------- coverage: platform win32, python 3.11.5 ----------
Name                              Stmts   Miss  Cover   Missing
---------------------------------------------------------------
src/__init__.py                       0      0   100%
src/pipeline.py                     145     12    92%   234-245
src/utils.py                         89      5    94%   112-116
src/data/__init__.py                  0      0   100%
src/data/preprocessor.py             82      3    96%   78-80
src/data/transformer.py             269     18    93%   145-163
src/models/__init__.py                0      0   100%
src/models/trainer.py               156     10    94%   223-232
src/models/evaluator.py              54      2    96%   48-49
src/models/optimizer.py              67      4    94%   59-62
src/ops/__init__.py                   0      0   100%
src/ops/dataops.py                   98      6    94%   85-90
src/ops/mlops.py                    485     35    93%   Multiple lines
src/visualization/__init__.py         0      0   100%
src/visualization/eda_plots.py      112      8    93%   89-96
src/visualization/evaluate_plots.py 128     11    91%   105-115
---------------------------------------------------------------
TOTAL                              1685    114    93%
```

#### 6ï¸âƒ£ Cháº¡y tests nhanh (skip slow tests)

```powershell
# Skip tests Ä‘Æ°á»£c mark lÃ  slow
pytest -m "not slow"

# Chá»‰ cháº¡y fast tests
pytest -m fast
```

#### 7ï¸âƒ£ Cháº¡y tests vá»›i output chi tiáº¿t

```powershell
# Hiá»ƒn thá»‹ print statements
pytest -s

# Hiá»ƒn thá»‹ local variables khi fail
pytest -l

# Stop sau test fail Ä‘áº§u tiÃªn
pytest -x

# Stop sau N failures
pytest --maxfail=3
```

#### 8ï¸âƒ£ Cháº¡y tests song song (nhanh hÆ¡n)

```powershell
# CÃ i pytest-xdist
pip install pytest-xdist

# Cháº¡y vá»›i N workers
pytest -n 4

# Cháº¡y vá»›i auto workers (dá»±a trÃªn CPU cores)
pytest -n auto
```

#### 9ï¸âƒ£ Táº¡o test report

```powershell
# XML report (cho CI/CD)
pytest --junitxml=test-results.xml

# HTML report
pip install pytest-html
pytest --html=test-report.html --self-contained-html
```

#### ğŸ”Ÿ Watch mode (auto-rerun khi code thay Ä‘á»•i)

```powershell
# CÃ i pytest-watch
pip install pytest-watch

# Auto-rerun tests
ptw
```

---

### ğŸ“Š Test Coverage Goals

| Module | Target Coverage | Current | Status |
|--------|----------------|---------|--------|
| `src/data/` | â‰¥ 90% | 94% | âœ… |
| `src/models/` | â‰¥ 90% | 94% | âœ… |
| `src/ops/` | â‰¥ 85% | 93% | âœ… |
| `src/visualization/` | â‰¥ 80% | 92% | âœ… |
| `src/utils.py` | â‰¥ 95% | 94% | âš ï¸ |
| **Overall** | **â‰¥ 90%** | **93%** | **âœ…** |

---

### ğŸ” Test Examples

#### Example 1: Test Data Processing
```python
# tests/test_data_processing.py
def test_load_data(sample_raw_data):
    """Test loading raw data"""
    assert sample_raw_data.shape[0] == 5
    assert 'target' in sample_raw_data.columns

def test_clean_data(sample_raw_data):
    """Test data cleaning"""
    # Remove duplicates
    cleaned = preprocessor.clean_data(sample_raw_data)
    assert cleaned.shape[0] <= sample_raw_data.shape[0]
```

#### Example 2: Test Feature Engineering
```python
# tests/test_feature_engineering.py
def test_fit_transform():
    """Test transformer fit and transform"""
    X_train, y_train = transformer.fit_transform(train_df)
    assert X_train.shape[0] == train_df.shape[0]
    assert y_train is not None

def test_transform_test():
    """Test transformer only transforms test"""
    X_test, y_test = transformer.transform(test_df)
    # Should use fitted params, not refit
    assert X_test.shape[1] == X_train.shape[1]
```

#### Example 3: Test Model Training
```python
# tests/test_model_training.py
def test_train_xgboost():
    """Test XGBoost training"""
    trainer.load_train_test_data(train_path, test_path)
    trainer.train_model('xgboost')
    assert trainer.models['xgboost'] is not None

def test_evaluate_model():
    """Test model evaluation"""
    metrics = trainer.evaluate('xgboost')
    assert 'f1' in metrics['metrics']
    assert metrics['metrics']['f1'] > 0
```

---

### ğŸ› Debugging Tests

```powershell
# Cháº¡y vá»›i debugger
pytest --pdb

# Drop vÃ o debugger khi fail
pytest --pdb --pdbcls=IPython.terminal.debugger:Pdb

# Cháº¡y last failed tests only
pytest --lf

# Cháº¡y failed tests trÆ°á»›c, sau Ä‘Ã³ má»›i cháº¡y passed
pytest --ff
```

---

### âœ… Best Practices

1. **Cháº¡y tests trÆ°á»›c khi commit**
   ```powershell
   pytest --cov=src --cov-report=term-missing
   ```

2. **Kiá»ƒm tra coverage Ä‘áº§y Ä‘á»§**
   ```powershell
   pytest --cov=src --cov-report=html
   # Review htmlcov/index.html
   ```

3. **Test trÃªn nhiá»u mÃ´i trÆ°á»ng**
   ```powershell
   # Test vá»›i Python 3.9, 3.10, 3.11
   tox  # (náº¿u cÃ³ tox.ini)
   ```

4. **CI/CD Integration**
   ```yaml
   # .github/workflows/tests.yml
   - name: Run tests
     run: |
       pytest --cov=src --cov-report=xml
       pytest --junitxml=test-results.xml
   ```

---

## ğŸ”„ Workflow thá»±c táº¿ (Real-world Workflow)

### Scenario 1: KhÃ¡m phÃ¡ dá»¯ liá»‡u má»›i

```powershell
# 1. Clone vÃ  setup
git clone https://github.com/civi0411/Churn_Analys_and_Prediction.git
cd Churn_Analys_and_Prediction
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt

# 2. Äáº·t file data má»›i vÃ o thÆ° má»¥c
# Copy your_data.xlsx â†’ data/raw/

# 3. EDA Ä‘á»ƒ hiá»ƒu dá»¯ liá»‡u
python main.py --mode eda --data "data/raw/your_data.xlsx"

# 4. Xem káº¿t quáº£ trong artifacts/experiments/<run_id>/figures/eda/
```

### Scenario 2: Training model cho production

```powershell
# 1. Chá»‰nh config cho production
# Edit config/config.yaml:
#   - TÄƒng n_iter cho tuning
#   - Chá»n models phÃ¹ há»£p
#   - Báº­t monitoring

# 2. Cháº¡y full pipeline vá»›i optimize
python main.py --mode full --model xgboost --optimize

# 3. Kiá»ƒm tra káº¿t quáº£
# - artifacts/experiments/<run_id>/metrics.json
# - artifacts/model_registry/xgboost_v*.joblib

# 4. Deploy model
# Copy model tá»« registry ra production server
```

### Scenario 3: So sÃ¡nh nhiá»u models

```powershell
# 1. Train táº¥t cáº£ models vá»›i tuning
python main.py --mode train --model all --optimize

# 2. Xem model comparison chart
# artifacts/experiments/<run_id>/figures/evaluation/model_comparison.png

# 3. Select best model tá»« log
# Check logs hoáº·c metrics.json
```

### Scenario 4: Monitoring vÃ  retraining

```powershell
# 1. Kiá»ƒm tra performance log
# Review artifacts/monitoring/performance_log.csv

# 2. Náº¿u phÃ¡t hiá»‡n drift, retrain vá»›i data má»›i
python main.py --mode full --model xgboost --optimize --data "data/raw/new_batch.xlsx"

# 3. Compare metrics vá»›i version cÅ©
# So sÃ¡nh registry.json vÃ  performance_log.csv
```

---

## ğŸ› ï¸ Troubleshooting (Xá»­ lÃ½ lá»—i thÆ°á»ng gáº·p)

### âŒ Lá»—i: `FileNotFoundError: data/raw/E Commerce Dataset.xlsx`

**NguyÃªn nhÃ¢n:** File data khÃ´ng tá»“n táº¡i

**Giáº£i phÃ¡p:**
```powershell
# Kiá»ƒm tra file cÃ³ tá»“n táº¡i
ls data/raw/

# Náº¿u file cÃ³ tÃªn khÃ¡c, sá»­ dá»¥ng --data
python main.py --mode eda --data "data/raw/your_actual_filename.xlsx"

# Hoáº·c sá»­a config.yaml
# data:
#   raw_path: "data/raw/your_actual_filename.xlsx"
```

### âŒ Lá»—i: `KeyError: 'Churn'`

**NguyÃªn nhÃ¢n:** Cá»™t target khÃ´ng tá»“n táº¡i trong data

**Giáº£i phÃ¡p:**
```powershell
# Kiá»ƒm tra tÃªn cá»™t trong Excel/CSV
# Sá»­a config.yaml:
data:
  target_col: "YourActualTargetColumn"  # VÃ­ dá»¥: "Churned", "Exited", "Left"
```

### âŒ Lá»—i: SMOTE requires `n_neighbors <= n_samples`

**NguyÃªn nhÃ¢n:** Dá»¯ liá»‡u quÃ¡ Ã­t cho SMOTE

**Giáº£i phÃ¡p:**
```yaml
# Sá»­a config.yaml
preprocessing:
  use_smote: false  # Táº¯t SMOTE
  # Hoáº·c giáº£m k_neighbors
  k_neighbors: 3    # Thay vÃ¬ 5
```

### âŒ Lá»—i: `OutOfMemoryError` khi train

**NguyÃªn nhÃ¢n:** Model hoáº·c data quÃ¡ lá»›n

**Giáº£i phÃ¡p:**
```yaml
# Giáº£m complexity cá»§a models trong config.yaml
models:
  xgboost:
    n_estimators: [50, 100]      # Thay vÃ¬ [100, 300, 500]
    max_depth: [3, 5]            # Thay vÃ¬ [3, 5, 7]

# Hoáº·c giáº£m n_iter cho tuning
tuning:
  n_iter: 10                     # Thay vÃ¬ 20
```

### âŒ Lá»—i: Tests fail

**Giáº£i phÃ¡p:**
```powershell
# 1. Cháº¡y tests vá»›i verbose Ä‘á»ƒ xem lá»—i chi tiáº¿t
pytest -v -s

# 2. Cháº¡y test cá»¥ thá»ƒ bá»‹ fail
pytest tests/test_data_processing.py::test_failing_function -v

# 3. Check dependencies
pip install -r requirements.txt --upgrade

# 4. Clear cache vÃ  rerun
pytest --cache-clear
```

---

## ğŸ“ˆ Káº¿t quáº£ máº«u (Sample Results)

### Model Performance Comparison

| Model | F1-Score | ROC-AUC | Precision | Recall | Training Time |
|-------|----------|---------|-----------|--------|---------------|
| XGBoost | **0.9123** | **0.9456** | 0.8945 | 0.9312 | 45.2s |
| Random Forest | 0.8876 | 0.9234 | 0.8723 | 0.9034 | 32.1s |
| Logistic Regression | 0.8234 | 0.8756 | 0.8012 | 0.8467 | 2.3s |
| SVM | 0.8456 | 0.8923 | 0.8234 | 0.8689 | 78.5s |
| Decision Tree | 0.7989 | 0.8234 | 0.7756 | 0.8234 | 1.8s |
| AdaBoost | 0.8567 | 0.9012 | 0.8345 | 0.8801 | 28.4s |

> ğŸ† **Best Model:** XGBoost vá»›i F1-Score = 0.9123

### Feature Importance (Top 10)

```
1. DaySinceLastOrder        0.1823
2. CashbackAmount           0.1456
3. OrderCount               0.1234
4. Tenure                   0.1123
5. CouponUsed               0.0945
6. ComplainStatus           0.0876
7. OrderAmountHikeFromlastYear  0.0756
8. WarehouseToHome          0.0645
9. DaySinceLastOrder_Log    0.0534
10. SatisfactionScore       0.0423
```

---

## ğŸ¤ Contributing (ÄÃ³ng gÃ³p)

Contributions are welcome! Náº¿u báº¡n muá»‘n Ä‘Ã³ng gÃ³p:

1. Fork repository
2. Táº¡o branch má»›i: `git checkout -b feature/your-feature`
3. Commit changes: `git commit -m 'Add some feature'`
4. Push to branch: `git push origin feature/your-feature`
5. Táº¡o Pull Request

### Development Guidelines

- âœ… Viáº¿t tests cho code má»›i (coverage â‰¥ 90%)
- âœ… Follow PEP 8 style guide
- âœ… ThÃªm docstrings cho functions/classes
- âœ… Cáº­p nháº­t README náº¿u thÃªm features má»›i

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“ Contact & Support

- **Author:** [civi0411](https://github.com/civi0411)
- **Repository:** [Churn_Analys_and_Prediction](https://github.com/civi0411/Churn_Analys_and_Prediction)
- **Issues:** [GitHub Issues](https://github.com/civi0411/Churn_Analys_and_Prediction/issues)

---

<p align="center">
  <b>Made with â¤ï¸ by <a href="https://github.com/civi0411">civi0411</a></b><br>
  <i>Data Science â€¢ Machine Learning â€¢ MLOps</i>
</p>

